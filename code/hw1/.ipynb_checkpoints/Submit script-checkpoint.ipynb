{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bengaliai-cv19/train.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_2.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\n",
      "/kaggle/input/bengaliai-cv19/class_map.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_3.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_1.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\n",
      "/kaggle/input/bengaliai-cv19/test.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_0.parquet\n",
      "/kaggle/input/bengaliai-cv19/sample_submission.csv\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\n",
      "/kaggle/input/models/model-best-128.h5\n",
      "/kaggle/input/models/model-best-64.h5\n",
      "/kaggle/input/models/model-best-96.h5\n",
      "/kaggle/input/models/model-best-32.h5\n",
      "Loading /kaggle/input/models/model-best-32.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess arguments:\n",
      "{'image_width': 42, 'image_height': 42, 'padding': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /kaggle/input/models/model-best-64.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess arguments:\n",
      "{'image_width': 74, 'image_height': 74, 'padding': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       row_id  target\n",
      "0  Test_0_consonant_diacritic       0\n",
      "1        Test_0_grapheme_root       3\n",
      "2      Test_0_vowel_diacritic       0\n",
      "3  Test_1_consonant_diacritic       0\n",
      "4        Test_1_grapheme_root      93\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "from starter_eda_model_funcs import MultiOutputDataGenerator\n",
    "from helper import to_one_hot, preview_data_aug\n",
    "from preprocessing import perform_preprocessing\n",
    "from flow_kaggle import generators_from_prep, generator_wrapper\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Test submission code from https://www.kaggle.com/ipythonx/keras-grapheme-gridmask-augmix-in-efficientnet\n",
    "# Test data generator\n",
    "def test_batch_generator(df, batch_size, SIZE, PAD, HEIGHT=137, WIDTH=236):\n",
    "    num_imgs = len(df)\n",
    "\n",
    "    for batch_start in range(0, num_imgs, batch_size):\n",
    "        curr_batch_size = min(num_imgs, batch_start + batch_size) - batch_start\n",
    "        idx = np.arange(batch_start, batch_start + curr_batch_size)\n",
    "\n",
    "        names_batch = df.iloc[idx, 0].values\n",
    "        imgs_batch = 255 - df.iloc[idx, 1:].values.reshape(-1, HEIGHT, WIDTH).astype(np.uint8)\n",
    "        X_batch = np.zeros((curr_batch_size, SIZE, SIZE, 1))\n",
    "\n",
    "        for j in range(curr_batch_size):\n",
    "            img = (imgs_batch[j,]*(255.0/imgs_batch[j,].max())).astype(np.uint8)\n",
    "            img = crop_resize(img, orig_height=HEIGHT, orig_width=WIDTH, target_height=SIZE, target_width=SIZE, pad=PAD)\n",
    "            img = img[:, :, np.newaxis]\n",
    "            X_batch[j,] = img\n",
    "\n",
    "        yield X_batch, names_batch\n",
    "\n",
    "VERBOSE = True\n",
    "\n",
    "\n",
    "# locate the parquet files \n",
    "TEST = [\n",
    "    \"../input/bengaliai-cv19/test_image_data_0.parquet\",\n",
    "    \"../input/bengaliai-cv19/test_image_data_1.parquet\",\n",
    "    \"../input/bengaliai-cv19/test_image_data_2.parquet\",\n",
    "    \"../input/bengaliai-cv19/test_image_data_3.parquet\",\n",
    "]\n",
    "for fn in TEST:\n",
    "    assert os.path.exists(fn), \"Test parquet file not found!\"\n",
    "\n",
    "roi_options = [32, 64, 96, 128]\n",
    "pad_options = [5, 5, 15,  21]\n",
    "# roi_options = [32, 64]\n",
    "# pad_options = [5, 5]\n",
    "batch_size = 256\n",
    "# models      = []\n",
    "\n",
    "\n",
    "from preprocessing import bbox, crop_resize\n",
    "# combine models to an ensemble\n",
    "probs = {}\n",
    "\n",
    "# --- PERFORM PREPROCESSING -> PREDICT -> CLEANUP -> repeat ---#\n",
    "for roi_size, padding in zip(roi_options, pad_options):\n",
    "    model_fn = f'/kaggle/input/models/model-best-{roi_size}.h5'\n",
    "    print(f\"Loading {model_fn}...\")\n",
    "    model = load_model(model_fn)\n",
    "    \n",
    "    preprocess_args = dict(\n",
    "        image_width=roi_size + padding*2,\n",
    "        image_height=roi_size + padding*2,\n",
    "        padding=padding,\n",
    "    )\n",
    "    \n",
    "    # folders\n",
    "    data_path='/kaggle/input/bengaliai-cv19/'\n",
    "    name = f\"prep-{roi_size}-{padding}\"\n",
    "    prep_path = f\"prep/{name}/\"\n",
    "    train_or_test = 'test'\n",
    "    \n",
    "    name = f\"test_{roi_size}_{padding}/\"\n",
    "    # perform preprocessing\n",
    "    print(\"Preprocess arguments:\")\n",
    "    print(preprocess_args)\n",
    "#     perform_preprocessing(preprocess_args, out='png', prep_path=prep_path,\n",
    "#                           train_or_test=train_or_test, data_path=data_path)\n",
    "\n",
    "#     # get test generator\n",
    "#     _, test_generator = generators_from_prep(datagen_args={}, preprocess_args=preprocess_args, \n",
    "#                                           show_data_aug=True, data_path=data_path, \n",
    "#                                           prep_path=prep_path, train_or_test=train_or_test)\n",
    "    \n",
    "#     # predict test set\n",
    "#     y_preds = model.predict(generator_wrapper(test_generator), verbose=0, \n",
    "#                             steps=int(np.ceil(test_generator.n / test_generator.batch_size)))\n",
    "#     # need to round up to ensure all validation samples are yielded\n",
    "#     y_pred_list.append(y_preds.copy())\n",
    "\n",
    "    # placeholders \n",
    "    row_id = []\n",
    "    target = []\n",
    "\n",
    "    # iterative over the test sets\n",
    "    for fname in tqdm(TEST):\n",
    "        test_ = pd.read_parquet(fname)\n",
    "        assert preprocess_args['image_width'] == preprocess_args['image_height']\n",
    "        test_gen = test_batch_generator(test_, batch_size=batch_size, SIZE=preprocess_args['image_width'], PAD=preprocess_args['padding'])\n",
    "\n",
    "        for batch_x, batch_name in test_gen:\n",
    "            batch_predict = model.predict(batch_x)\n",
    "            for idx, name in enumerate(batch_name):\n",
    "                # save probabilities\n",
    "                if f\"{name}_consonant_diacritic\" in probs.keys():\n",
    "                    probs[f\"{name}_consonant_diacritic\"] += batch_predict[2][idx]\n",
    "                    probs[f\"{name}_grapheme_root\"] += batch_predict[0][idx]\n",
    "                    probs[f\"{name}_vowel_diacritic\"] += batch_predict[1][idx]\n",
    "                else:\n",
    "                    probs[f\"{name}_consonant_diacritic\"] = batch_predict[2][idx]\n",
    "                    probs[f\"{name}_grapheme_root\"] = batch_predict[0][idx]\n",
    "                    probs[f\"{name}_vowel_diacritic\"] = batch_predict[1][idx]\n",
    "                \n",
    "#                 row_id += [\n",
    "#                     f\"{name}_consonant_diacritic\",\n",
    "#                     f\"{name}_grapheme_root\",\n",
    "#                     f\"{name}_vowel_diacritic\",\n",
    "#                 ]\n",
    "#                 target += [\n",
    "#                     np.argmax(batch_predict[2], axis=1)[idx],\n",
    "#                     np.argmax(batch_predict[0], axis=1)[idx],\n",
    "#                     np.argmax(batch_predict[1], axis=1)[idx],\n",
    "#                 ]\n",
    "\n",
    "        del test_\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "#     df_sample = pd.DataFrame(\n",
    "#         {\n",
    "#             'row_id': row_id,\n",
    "#             'target': target\n",
    "#         },\n",
    "#         columns = ['row_id','target'] \n",
    "#     )\n",
    "    \n",
    "    # cleanup\n",
    "    K.clear_session() # remove model for faster loading of next model\n",
    "    gc.collect()\n",
    "\n",
    "# --- COMBINE PREDICTIONS --- #\n",
    "for k, val in probs.items():\n",
    "    probs[k] = np.argmax(val)\n",
    "\n",
    "df_sample = pd.DataFrame(\n",
    "    {\n",
    "        'row_id': list(probs.keys()),\n",
    "        'target': list(probs.values())\n",
    "    },\n",
    "    columns = ['row_id','target'] \n",
    ")\n",
    "# y_pred_root = np.mean([y_pred[0] for y_pred in y_pred_list], axis=0)\n",
    "# y_pred_vowe = np.mean([y_pred[1] for y_pred in y_pred_list], axis=0)\n",
    "# y_pred_cons = np.mean([y_pred[2] for y_pred in y_pred_list], axis=0)\n",
    "\n",
    "df_sample.to_csv('submission.csv', index=False)\n",
    "print(df_sample.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
